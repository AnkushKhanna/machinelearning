scala> df.describe("FinelineNumber").show
+-------+-----------------+
|summary|   FinelineNumber|
+-------+-----------------+
|  count|           642925|
|   mean|3726.884566629078|
| stddev|2780.963994776838|
|    min|                0|
|    max|             9998|
+-------+-----------------+

val df = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("/Users/ankushkhanna/Documents/kaggle/walmart/code4/src/main/resources/train.csv").toDF()

val dfTest = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("/Users/ankushkhanna/Documents/kaggle/walmart/code4/src/main/resources/test.csv").toDF()

val con = udf ((dd:String) => -1)
val df2 = dfTest.withColumn("TripType", con(dfTest.col("DepartmentDescription")))

val columns = df.columns.toSet.intersect(df2.columns.toSet).map(col).toSeq

val dfC = df.select(columns: _*).unionAll(df2.select(columns: _*))



val dfC2 = dfC.filter(dfC("FinelineNumber").isNotNull && dfC("finelineNumber") !== 0)

val fnC = dfC2.groupBy("FinelineNumber").count


val toDouble = udf ((count: Long) => count.toDouble)
val fnC2 = fnC.withColumn("count_d", toDouble(fnC.col("count")))

val arr1 = 0.0 to 1000.0 by 500.0 toArray

val arr2 = 1001.0 to 5000.0 by 1000.0 toArray

val arr3 = 5001.0 to 9000.0 by 2000.0 toArray

val x = arr1 ++ arr2 ++ arr3 :+ 11000.0 :+ 17000.0

import org.apache.spark.ml.feature.Bucketizer
val bucketizer = new Bucketizer().
       setInputCol("count_d").
       setOutputCol("buck_count").
        setSplits(x)

val bucketedData = bucketizer.transform(fnC2)


import java.io._
import scala.collection.mutable.Map
val dfFinal = bucketedData.select("FinelineNumber", "buck_count")
val list = dfFinal.rdd.collect
val finelineMap = Map[String, Double]()
list.map(r=> finelineMap.put(r.getAs[String](0), r.getAs[Double](1)))

val oos = new ObjectOutputStream(new FileOutputStream("/Users/ankushkhanna/Documents/kaggle/walmart/code4/src/main/resources/FineLineBuck"))
oos.writeObject(finelineMap)
oos.flush
oos.close

+-------+------------------+
|summary|    FinelineNumber|
+-------+------------------+
|  count|           1292585|
|   mean| 3726.058594212373|
| stddev|2778.7834941867004|
|    min|                 0|
|    max|              9999|
+-------+------------------+


val fnC = dfC.groupBy("FinelineNumber").count

val fnC = dfC.groupBy("FinelineNumber").count.orderBy(asc("count")).show

"""" Try and find out if less is more beneficial for you or more. 
	 Group by fineline number count and see if that gives some very specific values.cd 
""""

+--------------+-----+
|FinelineNumber|count|
+--------------+-----+
|          5501|16429|
|          1508|10310|
|           135| 8989|
|           808| 8879|
|          null| 8115|
|             0| 7542|
|          9546| 5984|
|          1407| 5877|
|          4606| 5469|
|           115| 5379|
|           203| 5255|
|          3004| 5235|
|           100| 5157|
|          4010| 5101|
|          3600| 4960|
|          3601| 4929|
|           110| 4483|
|          3555| 4136|
|          8101| 4122|
|          3120| 4121|
+--------------+-----+

val toDouble = udf ((dd: Int) => dd.toDouble)
val dfC2 = dfC.withColumn("FinelineNumberDouble", toDouble(dfC.col("FinelineNumber")))

val x = 0.0 to 9999.0 by 300.0 toArray

val bucketizer = new Bucketizer().
     | setInputCol("FinelineNumberDouble").
     | setOutputCol("buck_FinelineNumber").
     | setSplits(x)

val bucketedData = bucketizer.transform(dfC2)



//scala> dfC.describe("FinelineNumber").show
